# Sprint Name 

EYRA Benchmark Platform

# Team leader

AdriÃ«nne Mendrik

# Target project 

Enlighten Your ReseArch (EYRA) Benchmark Platform (NLeSC-Surf Alliance project 2018)

Abstract:
Manual analysis of research data is often tedious and expensive, and with the advent of big data close to impossible. However, algorithms that automatically analyse data are not readily available to researchers who are not computer science experts. Furthermore, not all algorithms perform equally well on all types of data. Getting insight into algorithm performance with respect to specific research questions is essential, especially since using suboptimal algorithms could have huge consequences and even lead to false research findings. Benchmarks provide a way to quantify algorithm performance with respect to a research question. Although many researchers are enthusiastic about benchmarking their research in an open, collaborative platform that uses data, truth criteria and metrics, there is currently no platform to easily set up a research benchmark. In this alliance, SURF and the eScience Center join forces to develop a platform that provides both a software and hardware infrastructure for research benchmarks. For the proposed pilot platform we will collaborate with researchers from the four discipline areas as defined by the eScience Center (Environment & Sustainability, Humanities & Social Sciences, Life Sciences & eHealth, Physics & Beyond). By default, the benchmarking  platform will adhere to the FAIR principles with respect to data, ground truth, metrics, algorithms and benchmarks in general, and promote interdisciplinary collaboration and cross-fertilization. It provides researchers from all scientific disciplines with the tools and infrastructure to easily set-up an open benchmark to identify the best performing algorithm for their specific research problem, and computer scientists with an environment to test their algorithms on a wide variety of relevant applications.

# Expertise required

- Software architect
- Containerization
- Data management
- CWL or similar

# Size of team

5

# Description

For the sprint the plan is to focus on the back-end of the benchmark platform (the front-end could be part of a later sprint):
- Make an inventory of the platforms and open source software currently available (Crowd AI, Kaggle, Comic, Covalic, Visceral, Evidencio) and that we could reuse (connect to). Also assess how they are different from what we are planning to do. 
- Brainstorm about the design of the platform taking different expertises into account
- Look into COMIC (https://grand-challenge.org/Create_your_own_challenge/) and see whether we could extent this (I already have commitment from the developers of this platform that they are willing to collaborate): https://github.com/comic/grand-challenge.org (I could ask whether James (research software engineer from Radboud UMC who is developing this, could join the sprint)

# Goals

- First draft describing all aspects of the platform design and hardware demands for SURF. 
- Overview of existing platforms and open source software, and how these differ or could be re-used



